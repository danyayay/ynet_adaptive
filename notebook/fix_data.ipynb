{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch \n",
    "import random\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    cv2.setRNGSeed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_df_meta_ids(df, meta_ids):\n",
    "    return df[(df[\"metaId\"].values == meta_ids[:, None]).sum(axis=0).astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low to high \n",
    "list_n_sample = [10, 20, 40, 80]\n",
    "vel_range = '4_8'\n",
    "save_dir = f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}'\n",
    "pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "n_val = 50\n",
    "n_test = 250\n",
    "df = pd.read_pickle(f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}.pkl')\n",
    "unique_meta_ids = np.unique(df[\"metaId\"])\n",
    "set_random_seeds(1)\n",
    "np.random.shuffle(unique_meta_ids)\n",
    "n_data = unique_meta_ids.shape[0]\n",
    "n_train = n_data - n_val - n_test\n",
    "train_meta_ids, val_meta_ids, test_meta_ids = \\\n",
    "    np.split(unique_meta_ids, [n_train, n_train + n_val])\n",
    "df_train = reduce_df_meta_ids(df, train_meta_ids)\n",
    "df_val = reduce_df_meta_ids(df, val_meta_ids)\n",
    "df_test = reduce_df_meta_ids(df, test_meta_ids)\n",
    "df_train.to_pickle(f'{save_dir}/train.pkl')\n",
    "df_val.to_pickle(f'{save_dir}/val.pkl')\n",
    "df_test.to_pickle(f'{save_dir}/test.pkl')\n",
    "for n_sample in list_n_sample:\n",
    "    df_sample = reduce_df_meta_ids(df, train_meta_ids[:n_sample])\n",
    "    df_sample.to_pickle(f'{save_dir}/train_{n_sample}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high to low \n",
    "vel_range = '0.5_2.25'\n",
    "save_dir = f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}'\n",
    "pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "n_val = 50\n",
    "n_test = 250\n",
    "df = pd.read_pickle(f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}.pkl')\n",
    "unique_meta_ids = np.unique(df[\"metaId\"])\n",
    "set_random_seeds(2)\n",
    "np.random.shuffle(unique_meta_ids)\n",
    "n_data = unique_meta_ids.shape[0]\n",
    "n_train = n_data - n_val - n_test\n",
    "train_meta_ids, val_meta_ids, test_meta_ids = \\\n",
    "    np.split(unique_meta_ids, [n_train, n_train + n_val])\n",
    "df_train = reduce_df_meta_ids(df, train_meta_ids)\n",
    "df_val = reduce_df_meta_ids(df, val_meta_ids)\n",
    "df_test = reduce_df_meta_ids(df, test_meta_ids)\n",
    "df_train.to_pickle(f'{save_dir}/train.pkl')\n",
    "df_val.to_pickle(f'{save_dir}/val.pkl')\n",
    "df_test.to_pickle(f'{save_dir}/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = pd.read_pickle(f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}_seed1/train.pkl')\n",
    "y2 = pd.read_pickle(f'{data_dir}/sdd/filter/avg_vel/dc_013/Biker/{vel_range}/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4928, 4940, 4953, 4999, 5005, 5030, 5032, 5101, 5240, 5466, 5470,\n",
       "        5472, 5643, 5650, 5651, 5652, 5696, 5710, 5711, 5722, 5954, 5969,\n",
       "        5976, 5979, 6060, 6161, 6280, 6372, 6373, 6376, 6381, 6383, 6388,\n",
       "        6477, 6482, 6590, 6622, 6631, 6642, 6721, 6722, 7103, 7202, 7233,\n",
       "        7260, 7268, 7277, 7292, 7352, 7443, 7447, 7514, 7602, 7620, 7636,\n",
       "        7678, 7767, 7868, 7869, 7872, 7902, 7957, 8075, 8091, 8128, 8132,\n",
       "        8327, 8354, 8418, 8511, 8513, 8514, 8587, 8589, 8596, 8643, 8654,\n",
       "        8696, 8739, 8769, 8801, 8805, 8923, 9002, 9058, 9061, 9128, 9129]),\n",
       " array([4907, 4927, 4954, 5030, 5070, 5108, 5109, 5118, 5145, 5179, 5240,\n",
       "        5359, 5384, 5470, 5471, 5472, 5474, 5507, 5508, 5617, 5654, 5721,\n",
       "        5793, 5817, 5843, 5954, 6066, 6076, 6186, 6187, 6235, 6236, 6313,\n",
       "        6373, 6473, 6482, 6487, 6586, 6590, 6593, 6597, 6598, 6612, 6628,\n",
       "        6665, 6722, 6758, 6905, 6993, 7114, 7199, 7202, 7213, 7214, 7272,\n",
       "        7277, 7292, 7308, 7446, 7447, 7448, 7474, 7545, 7611, 7620, 7628,\n",
       "        7636, 7765, 7769, 7870, 7872, 7970, 8027, 8132, 8328, 8354, 8382,\n",
       "        8511, 8516, 8535, 8576, 8641, 8665, 8680, 8804, 8917, 9073, 9129]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.metaId.unique(), y2.metaId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{data_dir}/sdd/filter/agent_type/deathCircle_0/Biker'\n",
    "pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "val_split = 80\n",
    "n_test = 500\n",
    "df = pd.read_pickle(f'{data_dir}/sdd/filter/agent_type/deathCircle_0/Biker.pkl')\n",
    "unique_meta_ids = np.unique(df[\"metaId\"])\n",
    "set_random_seeds(1)\n",
    "np.random.shuffle(unique_meta_ids)\n",
    "n_data = unique_meta_ids.shape[0]\n",
    "n_val = int(val_split) if val_split > 1 else int(val_split * n_data)\n",
    "n_train = n_data - n_val - n_test\n",
    "train_meta_ids, val_meta_ids, test_meta_ids = \\\n",
    "    np.split(unique_meta_ids, [n_train, n_train + n_val])\n",
    "df_train = reduce_df_meta_ids(df, train_meta_ids)\n",
    "df_val = reduce_df_meta_ids(df, val_meta_ids)\n",
    "df_test = reduce_df_meta_ids(df, test_meta_ids)\n",
    "df_train.to_pickle(f'{save_dir}/train.pkl')\n",
    "df_val.to_pickle(f'{save_dir}/val.pkl')\n",
    "df_test.to_pickle(f'{save_dir}/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'sdd__ynetmod__biker_low_to_high'\n",
    "model = torch.load(f'../ckpts/{file}.pt')\n",
    "mydict = {k:v for k,v in model.items() if 'segmentation' not in k}\n",
    "torch.save(mydict, f'../ckpts/{file}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder.scene_stages.0.0.weight', 'encoder.scene_stages.0.0.bias', 'encoder.scene_stages.1.1.weight', 'encoder.scene_stages.1.1.bias', 'encoder.scene_stages.1.3.weight', 'encoder.scene_stages.1.3.bias', 'encoder.scene_stages.2.1.weight', 'encoder.scene_stages.2.1.bias', 'encoder.scene_stages.2.3.weight', 'encoder.scene_stages.2.3.bias', 'encoder.motion_stages.0.0.weight', 'encoder.motion_stages.0.0.bias', 'encoder.motion_stages.1.1.weight', 'encoder.motion_stages.1.1.bias', 'encoder.motion_stages.1.3.weight', 'encoder.motion_stages.1.3.bias', 'encoder.motion_stages.2.1.weight', 'encoder.motion_stages.2.1.bias', 'encoder.motion_stages.2.3.weight', 'encoder.motion_stages.2.3.bias', 'encoder.fusion_stages.0.1.weight', 'encoder.fusion_stages.0.1.bias', 'encoder.fusion_stages.0.3.weight', 'encoder.fusion_stages.0.3.bias', 'encoder.fusion_stages.1.1.weight', 'encoder.fusion_stages.1.1.bias', 'encoder.fusion_stages.1.3.weight', 'encoder.fusion_stages.1.3.bias', 'goal_decoder.center.0.weight', 'goal_decoder.center.0.bias', 'goal_decoder.center.2.weight', 'goal_decoder.center.2.bias', 'goal_decoder.upsample_conv.0.weight', 'goal_decoder.upsample_conv.0.bias', 'goal_decoder.upsample_conv.1.weight', 'goal_decoder.upsample_conv.1.bias', 'goal_decoder.upsample_conv.2.weight', 'goal_decoder.upsample_conv.2.bias', 'goal_decoder.upsample_conv.3.weight', 'goal_decoder.upsample_conv.3.bias', 'goal_decoder.upsample_conv.4.weight', 'goal_decoder.upsample_conv.4.bias', 'goal_decoder.decoder.0.0.weight', 'goal_decoder.decoder.0.0.bias', 'goal_decoder.decoder.0.2.weight', 'goal_decoder.decoder.0.2.bias', 'goal_decoder.decoder.1.0.weight', 'goal_decoder.decoder.1.0.bias', 'goal_decoder.decoder.1.2.weight', 'goal_decoder.decoder.1.2.bias', 'goal_decoder.decoder.2.0.weight', 'goal_decoder.decoder.2.0.bias', 'goal_decoder.decoder.2.2.weight', 'goal_decoder.decoder.2.2.bias', 'goal_decoder.decoder.3.0.weight', 'goal_decoder.decoder.3.0.bias', 'goal_decoder.decoder.3.2.weight', 'goal_decoder.decoder.3.2.bias', 'goal_decoder.decoder.4.0.weight', 'goal_decoder.decoder.4.0.bias', 'goal_decoder.decoder.4.2.weight', 'goal_decoder.decoder.4.2.bias', 'goal_decoder.predictor.weight', 'goal_decoder.predictor.bias', 'traj_decoder.center.0.weight', 'traj_decoder.center.0.bias', 'traj_decoder.center.2.weight', 'traj_decoder.center.2.bias', 'traj_decoder.upsample_conv.0.weight', 'traj_decoder.upsample_conv.0.bias', 'traj_decoder.upsample_conv.1.weight', 'traj_decoder.upsample_conv.1.bias', 'traj_decoder.upsample_conv.2.weight', 'traj_decoder.upsample_conv.2.bias', 'traj_decoder.upsample_conv.3.weight', 'traj_decoder.upsample_conv.3.bias', 'traj_decoder.upsample_conv.4.weight', 'traj_decoder.upsample_conv.4.bias', 'traj_decoder.decoder.0.0.weight', 'traj_decoder.decoder.0.0.bias', 'traj_decoder.decoder.0.2.weight', 'traj_decoder.decoder.0.2.bias', 'traj_decoder.decoder.1.0.weight', 'traj_decoder.decoder.1.0.bias', 'traj_decoder.decoder.1.2.weight', 'traj_decoder.decoder.1.2.bias', 'traj_decoder.decoder.2.0.weight', 'traj_decoder.decoder.2.0.bias', 'traj_decoder.decoder.2.2.weight', 'traj_decoder.decoder.2.2.bias', 'traj_decoder.decoder.3.0.weight', 'traj_decoder.decoder.3.0.bias', 'traj_decoder.decoder.3.2.weight', 'traj_decoder.decoder.3.2.bias', 'traj_decoder.decoder.4.0.weight', 'traj_decoder.decoder.4.0.bias', 'traj_decoder.decoder.4.2.weight', 'traj_decoder.decoder.4.2.bias', 'traj_decoder.predictor.weight', 'traj_decoder.predictor.bias'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29439c2701c2c99292f4c27f6d5b631364f5a85a3fb1398d59a7605b4a1a4267"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ynetv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
